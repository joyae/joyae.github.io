---
layout: post
title: Character-Level Convolutional Networks(charCNN)
subtitle: Text Data + CNN
tags: [nlp, CNN, charCNN, paper review]
---
> 변형된 비속어를 잡기 위한 자소 기반 CNN 모델(수정 중인 포스트)

## Intro
현재 Deep Text Lab에서 악성댓글을 탐지하고 문체를 변환하는 프로젝트를 진행 중에 있다. 악성댓글을 탐지하는 단계에서 챌린지가 되는 부분이 2가지가 있는데, 하나는 사용자들이 일부러 **비속어를 변형시켜 사용한 형태(변형비속어)** 를 잡아내는 것이고, 다른 하나는 비속어가 없는 댓글이지만 **문맥적으로 악의가 담긴 댓글** 을 탐지하는 것이다. 이번 포스트에 다루는 charCNN은 **변형비속어** 를 잡아내기 위해 활용했던 모델이다.

프로젝트에서 다루는 온라인 댓글들은 일반적으로 정제된 문장과 다르다. 맞춤법 오류나 오타, 새로운 신조어나 유행어, 특정 커뮤니티에서 쓰이는 혐오 표현들이 가득하다. 또한 유저들은 댓글이 필터링되는 것을 막기 위해, 어떤 단어를 일부러 한글자씩 띄어쓴다거나 자음만 쓴다거나 발음과 유사한 숫자 및 알파벳을 활용하는 경우 등등 다양한 방법으로 댓글을 변형시키기도 한다.

이러한 댓글 데이터에서 비속어 유무 기반으로 악성 댓글 여부를 분류하고자 할 때, 비속어 사전을 사용한 rule-base 모델은 가장 간단하면서도 먼저 변형되지 않은 비속어를 솎아낼 때 효과적이다. 다만 rule-base 모델은 변형된 비속어나 사전에 없었던 새로운 비속어 및 혐오 표현들을 잡지 못한다. 그래서 변형된 비속어를 잡기 위해 다양한 방법들을 고안해보았는데 그 중 성능이 좋았던 모델이 이 charCNN 모델이다. 이 charCNN 모델은 [Nexon Developers Conference 2018에서 넥슨코리아 인텔리전스랩스 어뷰징탐지팀에서 발표한 욕설 탐지 방법](http://ndc.vod.nexoncdn.co.kr/NDC2018/slides/NDC2018_0033/index.html)에서도 사용되었다.

프로젝트에서 이 charCNN을 구현할 때, 참고한 세 가지 논문은 아래와 같다.
1. [Yoon Kim - Convolutional neural networks for sentence classification](https://www.aclweb.org/anthology/D14-1181/): 텍스트 데이터 분류에 CNN을 활용한 최초의 논문(한국인 분!)
2. [Zhang et al. - Character-level Convolutional Networks for Text Classification](https://arxiv.org/pdf/1509.01626.pdf): 단어 임베딩이 아닌 알파벳 단위의 input을 CNN에 학습시킨 분류 모델 제안
3. [모경현 et al. - 단어와 자소 기반 합성곱 신경망을 이용한 문서 분류](http://www.dbpia.co.kr/pdf/pdfView.do?nodeId=NODE07456973&mark=0&bookmarkCnt=0&ipRange=N&language=ko_KR): 한국어에 적용하여 자소 단위의 input을 CNN에 학습시킨 분류 모델 제안

이 논문들을 중심으로 내가 공부했던 내용과 구현했던 charCNN 모델, 이 학습된 charCNN 모델을 LIME으로 분석한 과정을 간략히 정리해보고자 한다.

## CNN with Text Data
> 텍스트 데이터에 CNN을 활용한 최초의 시도   

[Yoon Kim - Convolutional neural networks for sentence classification](https://www.aclweb.org/anthology/D14-1181/)은 텍스트 데이터에 CNN을 접목시킨 최초의 논문이다.

## Character-level CNN
> 기존의 워드 임베딩에서 벗어나, character 단위의 input을 고안
[Zhang et al. - Character-level Convolutional Networks for Text Classification](https://arxiv.org/pdf/1509.01626.pdf) 논문에서는 오직 '알파벳' 단위의 input만을 활용한 CNN을 제안한다.

기존 text classification에서 쓰이는 input은 '단어' 토큰에 기반되어있었다. n-grams과 같이 나열된 단어들의 조합을 사용한 모델이 성능이 좋았기 때문이다. 그러나 단어 토큰에 기반된 모델은 텍스트 데이터의 퀄리티(e.g. 오타나 띄어쓰기 오류)나 토크나이저 성능에 영향을 크게 받는다. 그래서 잘 정제되어있는 텍스트 데이터는 모델이 잘 학습할 수 있겠지만, 사람들이 실생활에서 직접 쓰는 글들, 맞춤법오류/오타/줄임말/유행어/이모티콘이 가득한 텍스트 데이터에는 맞지 않는 방법일 수 있다.

알파벳 단위의 input을 만들기 위해, 논문에서는 `quantization`이라는 번역하자면 `양자화` 과정을 거친다.

## 한국어 Character-level CNN
> character 단위의 input을 활용한 CNN 모델을 한국어에 어떻게 적용할 수 있을까?

## 우리 charCNN 모델
